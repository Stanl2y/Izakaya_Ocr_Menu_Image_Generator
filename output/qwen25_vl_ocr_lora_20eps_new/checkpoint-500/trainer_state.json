{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.154440154440154,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.15444015444015444,
      "grad_norm": 1.937883973121643,
      "learning_rate": 0.00019878787878787878,
      "loss": 1.669,
      "step": 5
    },
    {
      "epoch": 0.3088803088803089,
      "grad_norm": 4.380936622619629,
      "learning_rate": 0.00019727272727272728,
      "loss": 0.9679,
      "step": 10
    },
    {
      "epoch": 0.46332046332046334,
      "grad_norm": 0.939929723739624,
      "learning_rate": 0.00019575757575757577,
      "loss": 1.0969,
      "step": 15
    },
    {
      "epoch": 0.6177606177606177,
      "grad_norm": 2.4344303607940674,
      "learning_rate": 0.00019424242424242425,
      "loss": 1.0689,
      "step": 20
    },
    {
      "epoch": 0.7722007722007722,
      "grad_norm": 7.863749980926514,
      "learning_rate": 0.00019272727272727274,
      "loss": 0.9042,
      "step": 25
    },
    {
      "epoch": 0.9266409266409267,
      "grad_norm": 8.358198165893555,
      "learning_rate": 0.00019121212121212122,
      "loss": 0.7534,
      "step": 30
    },
    {
      "epoch": 1.0617760617760619,
      "grad_norm": 2.1958720684051514,
      "learning_rate": 0.00018969696969696972,
      "loss": 0.389,
      "step": 35
    },
    {
      "epoch": 1.2162162162162162,
      "grad_norm": 2.1640045642852783,
      "learning_rate": 0.0001881818181818182,
      "loss": 0.6642,
      "step": 40
    },
    {
      "epoch": 1.3706563706563706,
      "grad_norm": 2.5802056789398193,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.6228,
      "step": 45
    },
    {
      "epoch": 1.525096525096525,
      "grad_norm": 2.576450824737549,
      "learning_rate": 0.00018515151515151516,
      "loss": 0.564,
      "step": 50
    },
    {
      "epoch": 1.6795366795366795,
      "grad_norm": 3.1289687156677246,
      "learning_rate": 0.00018363636363636366,
      "loss": 0.6565,
      "step": 55
    },
    {
      "epoch": 1.833976833976834,
      "grad_norm": 2.693765640258789,
      "learning_rate": 0.00018212121212121213,
      "loss": 0.7704,
      "step": 60
    },
    {
      "epoch": 1.9884169884169884,
      "grad_norm": 0.7722026705741882,
      "learning_rate": 0.00018060606060606063,
      "loss": 0.3595,
      "step": 65
    },
    {
      "epoch": 2.1235521235521237,
      "grad_norm": 3.54801869392395,
      "learning_rate": 0.0001790909090909091,
      "loss": 0.5523,
      "step": 70
    },
    {
      "epoch": 2.277992277992278,
      "grad_norm": 2.2684314250946045,
      "learning_rate": 0.0001775757575757576,
      "loss": 0.3981,
      "step": 75
    },
    {
      "epoch": 2.4324324324324325,
      "grad_norm": 1.8890621662139893,
      "learning_rate": 0.00017606060606060607,
      "loss": 0.6042,
      "step": 80
    },
    {
      "epoch": 2.586872586872587,
      "grad_norm": 4.927379131317139,
      "learning_rate": 0.00017454545454545454,
      "loss": 0.4735,
      "step": 85
    },
    {
      "epoch": 2.741312741312741,
      "grad_norm": 1.9503182172775269,
      "learning_rate": 0.00017303030303030304,
      "loss": 0.4139,
      "step": 90
    },
    {
      "epoch": 2.8957528957528957,
      "grad_norm": 3.8281123638153076,
      "learning_rate": 0.0001715151515151515,
      "loss": 0.4493,
      "step": 95
    },
    {
      "epoch": 3.030888030888031,
      "grad_norm": 1.7289024591445923,
      "learning_rate": 0.00017,
      "loss": 0.2211,
      "step": 100
    },
    {
      "epoch": 3.1853281853281854,
      "grad_norm": 3.019663095474243,
      "learning_rate": 0.00016848484848484848,
      "loss": 0.347,
      "step": 105
    },
    {
      "epoch": 3.33976833976834,
      "grad_norm": 1.9790955781936646,
      "learning_rate": 0.00016696969696969698,
      "loss": 0.3851,
      "step": 110
    },
    {
      "epoch": 3.494208494208494,
      "grad_norm": 1.3909085988998413,
      "learning_rate": 0.00016545454545454545,
      "loss": 0.2978,
      "step": 115
    },
    {
      "epoch": 3.6486486486486487,
      "grad_norm": 1.7402585744857788,
      "learning_rate": 0.00016393939393939395,
      "loss": 0.281,
      "step": 120
    },
    {
      "epoch": 3.8030888030888033,
      "grad_norm": 2.7990994453430176,
      "learning_rate": 0.00016242424242424243,
      "loss": 0.3566,
      "step": 125
    },
    {
      "epoch": 3.9575289575289574,
      "grad_norm": 1.8215049505233765,
      "learning_rate": 0.00016090909090909092,
      "loss": 0.2962,
      "step": 130
    },
    {
      "epoch": 4.0926640926640925,
      "grad_norm": 1.3814170360565186,
      "learning_rate": 0.0001593939393939394,
      "loss": 0.3363,
      "step": 135
    },
    {
      "epoch": 4.2471042471042475,
      "grad_norm": 1.2074097394943237,
      "learning_rate": 0.0001578787878787879,
      "loss": 0.266,
      "step": 140
    },
    {
      "epoch": 4.401544401544402,
      "grad_norm": 2.5090436935424805,
      "learning_rate": 0.00015636363636363637,
      "loss": 0.2736,
      "step": 145
    },
    {
      "epoch": 4.555984555984556,
      "grad_norm": 1.563441514968872,
      "learning_rate": 0.00015484848484848487,
      "loss": 0.2352,
      "step": 150
    },
    {
      "epoch": 4.710424710424711,
      "grad_norm": 1.7887097597122192,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.1485,
      "step": 155
    },
    {
      "epoch": 4.864864864864865,
      "grad_norm": 4.395633697509766,
      "learning_rate": 0.0001518181818181818,
      "loss": 0.2293,
      "step": 160
    },
    {
      "epoch": 5.0,
      "grad_norm": 3.8564116954803467,
      "learning_rate": 0.0001503030303030303,
      "loss": 0.2092,
      "step": 165
    },
    {
      "epoch": 5.154440154440154,
      "grad_norm": 1.5257221460342407,
      "learning_rate": 0.00014878787878787878,
      "loss": 0.1441,
      "step": 170
    },
    {
      "epoch": 5.308880308880309,
      "grad_norm": 1.589884638786316,
      "learning_rate": 0.00014727272727272728,
      "loss": 0.0579,
      "step": 175
    },
    {
      "epoch": 5.463320463320463,
      "grad_norm": 1.3821030855178833,
      "learning_rate": 0.00014575757575757575,
      "loss": 0.1851,
      "step": 180
    },
    {
      "epoch": 5.617760617760617,
      "grad_norm": 2.772163152694702,
      "learning_rate": 0.00014424242424242425,
      "loss": 0.1639,
      "step": 185
    },
    {
      "epoch": 5.772200772200772,
      "grad_norm": 2.790433406829834,
      "learning_rate": 0.00014272727272727272,
      "loss": 0.1193,
      "step": 190
    },
    {
      "epoch": 5.926640926640927,
      "grad_norm": 1.6534171104431152,
      "learning_rate": 0.00014121212121212122,
      "loss": 0.1179,
      "step": 195
    },
    {
      "epoch": 6.061776061776062,
      "grad_norm": 1.2918407917022705,
      "learning_rate": 0.0001396969696969697,
      "loss": 0.2006,
      "step": 200
    },
    {
      "epoch": 6.216216216216216,
      "grad_norm": 0.6197134256362915,
      "learning_rate": 0.0001381818181818182,
      "loss": 0.0411,
      "step": 205
    },
    {
      "epoch": 6.370656370656371,
      "grad_norm": 1.511488437652588,
      "learning_rate": 0.00013666666666666666,
      "loss": 0.1015,
      "step": 210
    },
    {
      "epoch": 6.525096525096525,
      "grad_norm": 3.1427741050720215,
      "learning_rate": 0.00013515151515151516,
      "loss": 0.0794,
      "step": 215
    },
    {
      "epoch": 6.67953667953668,
      "grad_norm": 0.7023050785064697,
      "learning_rate": 0.00013363636363636366,
      "loss": 0.0366,
      "step": 220
    },
    {
      "epoch": 6.833976833976834,
      "grad_norm": 1.5574462413787842,
      "learning_rate": 0.00013212121212121213,
      "loss": 0.0657,
      "step": 225
    },
    {
      "epoch": 6.988416988416988,
      "grad_norm": 2.914130210876465,
      "learning_rate": 0.00013060606060606063,
      "loss": 0.0979,
      "step": 230
    },
    {
      "epoch": 7.123552123552123,
      "grad_norm": 0.4121617376804352,
      "learning_rate": 0.0001290909090909091,
      "loss": 0.0283,
      "step": 235
    },
    {
      "epoch": 7.277992277992278,
      "grad_norm": 1.1003451347351074,
      "learning_rate": 0.0001275757575757576,
      "loss": 0.0665,
      "step": 240
    },
    {
      "epoch": 7.4324324324324325,
      "grad_norm": 1.8565587997436523,
      "learning_rate": 0.00012606060606060605,
      "loss": 0.0564,
      "step": 245
    },
    {
      "epoch": 7.586872586872587,
      "grad_norm": 0.9518690705299377,
      "learning_rate": 0.00012454545454545455,
      "loss": 0.0333,
      "step": 250
    },
    {
      "epoch": 7.741312741312742,
      "grad_norm": 0.5057347416877747,
      "learning_rate": 0.00012303030303030302,
      "loss": 0.0508,
      "step": 255
    },
    {
      "epoch": 7.895752895752896,
      "grad_norm": 0.34932830929756165,
      "learning_rate": 0.00012151515151515152,
      "loss": 0.0216,
      "step": 260
    },
    {
      "epoch": 8.03088803088803,
      "grad_norm": 0.37979575991630554,
      "learning_rate": 0.00012,
      "loss": 0.0231,
      "step": 265
    },
    {
      "epoch": 8.185328185328185,
      "grad_norm": 0.6592047810554504,
      "learning_rate": 0.00011848484848484849,
      "loss": 0.0154,
      "step": 270
    },
    {
      "epoch": 8.339768339768339,
      "grad_norm": 0.37299907207489014,
      "learning_rate": 0.00011696969696969697,
      "loss": 0.019,
      "step": 275
    },
    {
      "epoch": 8.494208494208495,
      "grad_norm": 1.148485541343689,
      "learning_rate": 0.00011545454545454546,
      "loss": 0.0169,
      "step": 280
    },
    {
      "epoch": 8.64864864864865,
      "grad_norm": 0.5118783116340637,
      "learning_rate": 0.00011393939393939394,
      "loss": 0.0233,
      "step": 285
    },
    {
      "epoch": 8.803088803088803,
      "grad_norm": 2.3543617725372314,
      "learning_rate": 0.00011242424242424243,
      "loss": 0.0334,
      "step": 290
    },
    {
      "epoch": 8.957528957528957,
      "grad_norm": 1.6320973634719849,
      "learning_rate": 0.00011090909090909092,
      "loss": 0.0135,
      "step": 295
    },
    {
      "epoch": 9.092664092664092,
      "grad_norm": 0.29438695311546326,
      "learning_rate": 0.0001093939393939394,
      "loss": 0.0201,
      "step": 300
    },
    {
      "epoch": 9.247104247104247,
      "grad_norm": 0.2202029824256897,
      "learning_rate": 0.00010787878787878789,
      "loss": 0.0067,
      "step": 305
    },
    {
      "epoch": 9.4015444015444,
      "grad_norm": 0.21852731704711914,
      "learning_rate": 0.00010636363636363637,
      "loss": 0.0125,
      "step": 310
    },
    {
      "epoch": 9.555984555984557,
      "grad_norm": 0.2769756317138672,
      "learning_rate": 0.00010484848484848486,
      "loss": 0.0098,
      "step": 315
    },
    {
      "epoch": 9.71042471042471,
      "grad_norm": 0.1820734739303589,
      "learning_rate": 0.00010333333333333334,
      "loss": 0.0113,
      "step": 320
    },
    {
      "epoch": 9.864864864864865,
      "grad_norm": 0.19361646473407745,
      "learning_rate": 0.00010181818181818181,
      "loss": 0.0083,
      "step": 325
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.26992279291152954,
      "learning_rate": 0.0001003030303030303,
      "loss": 0.0076,
      "step": 330
    },
    {
      "epoch": 10.154440154440154,
      "grad_norm": 0.11378872394561768,
      "learning_rate": 9.87878787878788e-05,
      "loss": 0.0066,
      "step": 335
    },
    {
      "epoch": 10.308880308880308,
      "grad_norm": 0.17496271431446075,
      "learning_rate": 9.727272727272728e-05,
      "loss": 0.0065,
      "step": 340
    },
    {
      "epoch": 10.463320463320464,
      "grad_norm": 0.1805657148361206,
      "learning_rate": 9.575757575757576e-05,
      "loss": 0.0058,
      "step": 345
    },
    {
      "epoch": 10.617760617760618,
      "grad_norm": 0.16276535391807556,
      "learning_rate": 9.424242424242424e-05,
      "loss": 0.0053,
      "step": 350
    },
    {
      "epoch": 10.772200772200772,
      "grad_norm": 0.2472691684961319,
      "learning_rate": 9.272727272727273e-05,
      "loss": 0.007,
      "step": 355
    },
    {
      "epoch": 10.926640926640927,
      "grad_norm": 0.19247078895568848,
      "learning_rate": 9.121212121212121e-05,
      "loss": 0.0072,
      "step": 360
    },
    {
      "epoch": 11.061776061776062,
      "grad_norm": 0.1275079995393753,
      "learning_rate": 8.96969696969697e-05,
      "loss": 0.0064,
      "step": 365
    },
    {
      "epoch": 11.216216216216216,
      "grad_norm": 0.0988444983959198,
      "learning_rate": 8.818181818181818e-05,
      "loss": 0.0047,
      "step": 370
    },
    {
      "epoch": 11.37065637065637,
      "grad_norm": 0.1914709508419037,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.0053,
      "step": 375
    },
    {
      "epoch": 11.525096525096526,
      "grad_norm": 0.14507566392421722,
      "learning_rate": 8.515151515151515e-05,
      "loss": 0.0051,
      "step": 380
    },
    {
      "epoch": 11.67953667953668,
      "grad_norm": 0.12719953060150146,
      "learning_rate": 8.363636363636364e-05,
      "loss": 0.0044,
      "step": 385
    },
    {
      "epoch": 11.833976833976834,
      "grad_norm": 0.15382523834705353,
      "learning_rate": 8.212121212121212e-05,
      "loss": 0.0063,
      "step": 390
    },
    {
      "epoch": 11.988416988416988,
      "grad_norm": 0.12203650176525116,
      "learning_rate": 8.060606060606061e-05,
      "loss": 0.0051,
      "step": 395
    },
    {
      "epoch": 12.123552123552123,
      "grad_norm": 0.10643462091684341,
      "learning_rate": 7.90909090909091e-05,
      "loss": 0.0036,
      "step": 400
    },
    {
      "epoch": 12.277992277992277,
      "grad_norm": 0.1597539335489273,
      "learning_rate": 7.757575757575758e-05,
      "loss": 0.0048,
      "step": 405
    },
    {
      "epoch": 12.432432432432432,
      "grad_norm": 0.1090228408575058,
      "learning_rate": 7.606060606060607e-05,
      "loss": 0.0038,
      "step": 410
    },
    {
      "epoch": 12.586872586872587,
      "grad_norm": 0.13475996255874634,
      "learning_rate": 7.454545454545455e-05,
      "loss": 0.0037,
      "step": 415
    },
    {
      "epoch": 12.741312741312742,
      "grad_norm": 0.09485586732625961,
      "learning_rate": 7.303030303030304e-05,
      "loss": 0.0041,
      "step": 420
    },
    {
      "epoch": 12.895752895752896,
      "grad_norm": 0.09275797009468079,
      "learning_rate": 7.151515151515152e-05,
      "loss": 0.004,
      "step": 425
    },
    {
      "epoch": 13.03088803088803,
      "grad_norm": 0.08223078399896622,
      "learning_rate": 7e-05,
      "loss": 0.0047,
      "step": 430
    },
    {
      "epoch": 13.185328185328185,
      "grad_norm": 0.12014573812484741,
      "learning_rate": 6.848484848484848e-05,
      "loss": 0.0034,
      "step": 435
    },
    {
      "epoch": 13.339768339768339,
      "grad_norm": 0.07711341977119446,
      "learning_rate": 6.696969696969696e-05,
      "loss": 0.004,
      "step": 440
    },
    {
      "epoch": 13.494208494208495,
      "grad_norm": 0.10605651885271072,
      "learning_rate": 6.545454545454546e-05,
      "loss": 0.0033,
      "step": 445
    },
    {
      "epoch": 13.64864864864865,
      "grad_norm": 0.09688882529735565,
      "learning_rate": 6.393939393939395e-05,
      "loss": 0.0045,
      "step": 450
    },
    {
      "epoch": 13.803088803088803,
      "grad_norm": 0.08999601006507874,
      "learning_rate": 6.242424242424243e-05,
      "loss": 0.0034,
      "step": 455
    },
    {
      "epoch": 13.957528957528957,
      "grad_norm": 0.10067692399024963,
      "learning_rate": 6.090909090909091e-05,
      "loss": 0.0033,
      "step": 460
    },
    {
      "epoch": 14.092664092664092,
      "grad_norm": 0.10519877076148987,
      "learning_rate": 5.93939393939394e-05,
      "loss": 0.0034,
      "step": 465
    },
    {
      "epoch": 14.247104247104247,
      "grad_norm": 0.09448911994695663,
      "learning_rate": 5.787878787878788e-05,
      "loss": 0.004,
      "step": 470
    },
    {
      "epoch": 14.4015444015444,
      "grad_norm": 0.07546090334653854,
      "learning_rate": 5.636363636363636e-05,
      "loss": 0.0032,
      "step": 475
    },
    {
      "epoch": 14.555984555984557,
      "grad_norm": 0.10337676852941513,
      "learning_rate": 5.484848484848485e-05,
      "loss": 0.0027,
      "step": 480
    },
    {
      "epoch": 14.71042471042471,
      "grad_norm": 0.08639026433229446,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.0027,
      "step": 485
    },
    {
      "epoch": 14.864864864864865,
      "grad_norm": 0.09004044532775879,
      "learning_rate": 5.181818181818182e-05,
      "loss": 0.0036,
      "step": 490
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.11604640632867813,
      "learning_rate": 5.030303030303031e-05,
      "loss": 0.0031,
      "step": 495
    },
    {
      "epoch": 15.154440154440154,
      "grad_norm": 0.10679946094751358,
      "learning_rate": 4.878787878787879e-05,
      "loss": 0.003,
      "step": 500
    }
  ],
  "logging_steps": 5,
  "max_steps": 660,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.4806231001022464e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
