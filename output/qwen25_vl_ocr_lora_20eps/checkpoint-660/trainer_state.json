{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 660,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.15444015444015444,
      "grad_norm": 12.102875709533691,
      "learning_rate": 0.00019878787878787878,
      "loss": 16.6455,
      "step": 5
    },
    {
      "epoch": 0.3088803088803089,
      "grad_norm": 8.612836837768555,
      "learning_rate": 0.00019727272727272728,
      "loss": 15.4422,
      "step": 10
    },
    {
      "epoch": 0.46332046332046334,
      "grad_norm": 15.66960620880127,
      "learning_rate": 0.00019575757575757577,
      "loss": 13.8521,
      "step": 15
    },
    {
      "epoch": 0.6177606177606177,
      "grad_norm": 13.55010986328125,
      "learning_rate": 0.00019424242424242425,
      "loss": 11.5602,
      "step": 20
    },
    {
      "epoch": 0.7722007722007722,
      "grad_norm": 20.282821655273438,
      "learning_rate": 0.00019272727272727274,
      "loss": 9.9551,
      "step": 25
    },
    {
      "epoch": 0.9266409266409267,
      "grad_norm": 5.452322959899902,
      "learning_rate": 0.00019121212121212122,
      "loss": 9.0585,
      "step": 30
    },
    {
      "epoch": 1.0617760617760619,
      "grad_norm": 3.41253662109375,
      "learning_rate": 0.00018969696969696972,
      "loss": 8.592,
      "step": 35
    },
    {
      "epoch": 1.2162162162162162,
      "grad_norm": 3.463043212890625,
      "learning_rate": 0.0001881818181818182,
      "loss": 8.2397,
      "step": 40
    },
    {
      "epoch": 1.3706563706563706,
      "grad_norm": 2.4217941761016846,
      "learning_rate": 0.0001866666666666667,
      "loss": 7.9708,
      "step": 45
    },
    {
      "epoch": 1.525096525096525,
      "grad_norm": 1.3318054676055908,
      "learning_rate": 0.00018515151515151516,
      "loss": 7.7295,
      "step": 50
    },
    {
      "epoch": 1.6795366795366795,
      "grad_norm": 0.9145647883415222,
      "learning_rate": 0.00018363636363636366,
      "loss": 7.5062,
      "step": 55
    },
    {
      "epoch": 1.833976833976834,
      "grad_norm": 0.6141495704650879,
      "learning_rate": 0.00018212121212121213,
      "loss": 7.395,
      "step": 60
    },
    {
      "epoch": 1.9884169884169884,
      "grad_norm": 0.446422278881073,
      "learning_rate": 0.00018060606060606063,
      "loss": 7.3138,
      "step": 65
    },
    {
      "epoch": 2.1235521235521237,
      "grad_norm": 0.4206068217754364,
      "learning_rate": 0.0001790909090909091,
      "loss": 7.2753,
      "step": 70
    },
    {
      "epoch": 2.277992277992278,
      "grad_norm": 0.2825780510902405,
      "learning_rate": 0.0001775757575757576,
      "loss": 7.2564,
      "step": 75
    },
    {
      "epoch": 2.4324324324324325,
      "grad_norm": 0.2927824556827545,
      "learning_rate": 0.00017606060606060607,
      "loss": 7.2299,
      "step": 80
    },
    {
      "epoch": 2.586872586872587,
      "grad_norm": 0.4625915288925171,
      "learning_rate": 0.00017454545454545454,
      "loss": 7.2157,
      "step": 85
    },
    {
      "epoch": 2.741312741312741,
      "grad_norm": 0.30620449781417847,
      "learning_rate": 0.00017303030303030304,
      "loss": 7.1869,
      "step": 90
    },
    {
      "epoch": 2.8957528957528957,
      "grad_norm": 0.389639675617218,
      "learning_rate": 0.0001715151515151515,
      "loss": 7.1964,
      "step": 95
    },
    {
      "epoch": 3.030888030888031,
      "grad_norm": 0.25626951456069946,
      "learning_rate": 0.00017,
      "loss": 7.1793,
      "step": 100
    },
    {
      "epoch": 3.1853281853281854,
      "grad_norm": 0.20162326097488403,
      "learning_rate": 0.00016848484848484848,
      "loss": 7.1737,
      "step": 105
    },
    {
      "epoch": 3.33976833976834,
      "grad_norm": 0.26998186111450195,
      "learning_rate": 0.00016696969696969698,
      "loss": 7.1509,
      "step": 110
    },
    {
      "epoch": 3.494208494208494,
      "grad_norm": 0.3277542293071747,
      "learning_rate": 0.00016545454545454545,
      "loss": 7.1816,
      "step": 115
    },
    {
      "epoch": 3.6486486486486487,
      "grad_norm": 0.22031369805335999,
      "learning_rate": 0.00016393939393939395,
      "loss": 7.1644,
      "step": 120
    },
    {
      "epoch": 3.8030888030888033,
      "grad_norm": 0.3926728665828705,
      "learning_rate": 0.00016242424242424243,
      "loss": 7.1476,
      "step": 125
    },
    {
      "epoch": 3.9575289575289574,
      "grad_norm": 0.2644396722316742,
      "learning_rate": 0.00016090909090909092,
      "loss": 7.1517,
      "step": 130
    },
    {
      "epoch": 4.0926640926640925,
      "grad_norm": 0.5153000354766846,
      "learning_rate": 0.0001593939393939394,
      "loss": 7.1301,
      "step": 135
    },
    {
      "epoch": 4.2471042471042475,
      "grad_norm": 0.4845443665981293,
      "learning_rate": 0.0001578787878787879,
      "loss": 7.1528,
      "step": 140
    },
    {
      "epoch": 4.401544401544402,
      "grad_norm": 0.23729169368743896,
      "learning_rate": 0.00015636363636363637,
      "loss": 7.1247,
      "step": 145
    },
    {
      "epoch": 4.555984555984556,
      "grad_norm": 0.24302314221858978,
      "learning_rate": 0.00015484848484848487,
      "loss": 7.1316,
      "step": 150
    },
    {
      "epoch": 4.710424710424711,
      "grad_norm": 0.1946488618850708,
      "learning_rate": 0.00015333333333333334,
      "loss": 7.1217,
      "step": 155
    },
    {
      "epoch": 4.864864864864865,
      "grad_norm": 0.20341138541698456,
      "learning_rate": 0.0001518181818181818,
      "loss": 7.1351,
      "step": 160
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.271628201007843,
      "learning_rate": 0.0001503030303030303,
      "loss": 7.1243,
      "step": 165
    },
    {
      "epoch": 5.154440154440154,
      "grad_norm": 0.16180415451526642,
      "learning_rate": 0.00014878787878787878,
      "loss": 7.0952,
      "step": 170
    },
    {
      "epoch": 5.308880308880309,
      "grad_norm": 0.1443817913532257,
      "learning_rate": 0.00014727272727272728,
      "loss": 7.0973,
      "step": 175
    },
    {
      "epoch": 5.463320463320463,
      "grad_norm": 0.24130161106586456,
      "learning_rate": 0.00014575757575757575,
      "loss": 7.1157,
      "step": 180
    },
    {
      "epoch": 5.617760617760617,
      "grad_norm": 0.1984904259443283,
      "learning_rate": 0.00014424242424242425,
      "loss": 7.1252,
      "step": 185
    },
    {
      "epoch": 5.772200772200772,
      "grad_norm": 0.23059894144535065,
      "learning_rate": 0.00014272727272727272,
      "loss": 7.1282,
      "step": 190
    },
    {
      "epoch": 5.926640926640927,
      "grad_norm": 0.19540998339653015,
      "learning_rate": 0.00014121212121212122,
      "loss": 7.1137,
      "step": 195
    },
    {
      "epoch": 6.061776061776062,
      "grad_norm": 0.24431706964969635,
      "learning_rate": 0.0001396969696969697,
      "loss": 7.107,
      "step": 200
    },
    {
      "epoch": 6.216216216216216,
      "grad_norm": 0.2439107745885849,
      "learning_rate": 0.0001381818181818182,
      "loss": 7.1024,
      "step": 205
    },
    {
      "epoch": 6.370656370656371,
      "grad_norm": 0.5371281504631042,
      "learning_rate": 0.00013666666666666666,
      "loss": 7.114,
      "step": 210
    },
    {
      "epoch": 6.525096525096525,
      "grad_norm": 0.22551332414150238,
      "learning_rate": 0.00013515151515151516,
      "loss": 7.1181,
      "step": 215
    },
    {
      "epoch": 6.67953667953668,
      "grad_norm": 0.26938360929489136,
      "learning_rate": 0.00013363636363636366,
      "loss": 7.0813,
      "step": 220
    },
    {
      "epoch": 6.833976833976834,
      "grad_norm": 0.2873261272907257,
      "learning_rate": 0.00013212121212121213,
      "loss": 7.111,
      "step": 225
    },
    {
      "epoch": 6.988416988416988,
      "grad_norm": 0.2158125936985016,
      "learning_rate": 0.00013060606060606063,
      "loss": 7.0921,
      "step": 230
    },
    {
      "epoch": 7.123552123552123,
      "grad_norm": 0.21558085083961487,
      "learning_rate": 0.0001290909090909091,
      "loss": 7.1103,
      "step": 235
    },
    {
      "epoch": 7.277992277992278,
      "grad_norm": 0.21174290776252747,
      "learning_rate": 0.0001275757575757576,
      "loss": 7.0928,
      "step": 240
    },
    {
      "epoch": 7.4324324324324325,
      "grad_norm": 0.21746081113815308,
      "learning_rate": 0.00012606060606060605,
      "loss": 7.1063,
      "step": 245
    },
    {
      "epoch": 7.586872586872587,
      "grad_norm": 0.21839407086372375,
      "learning_rate": 0.00012454545454545455,
      "loss": 7.1063,
      "step": 250
    },
    {
      "epoch": 7.741312741312742,
      "grad_norm": 0.13775458931922913,
      "learning_rate": 0.00012303030303030302,
      "loss": 7.0685,
      "step": 255
    },
    {
      "epoch": 7.895752895752896,
      "grad_norm": 0.18025532364845276,
      "learning_rate": 0.00012151515151515152,
      "loss": 7.0928,
      "step": 260
    },
    {
      "epoch": 8.03088803088803,
      "grad_norm": 0.20766687393188477,
      "learning_rate": 0.00012,
      "loss": 7.1059,
      "step": 265
    },
    {
      "epoch": 8.185328185328185,
      "grad_norm": 0.2654268145561218,
      "learning_rate": 0.00011848484848484849,
      "loss": 7.0792,
      "step": 270
    },
    {
      "epoch": 8.339768339768339,
      "grad_norm": 0.21928179264068604,
      "learning_rate": 0.00011696969696969697,
      "loss": 7.0862,
      "step": 275
    },
    {
      "epoch": 8.494208494208495,
      "grad_norm": 0.17971006035804749,
      "learning_rate": 0.00011545454545454546,
      "loss": 7.1019,
      "step": 280
    },
    {
      "epoch": 8.64864864864865,
      "grad_norm": 0.22291339933872223,
      "learning_rate": 0.00011393939393939394,
      "loss": 7.0889,
      "step": 285
    },
    {
      "epoch": 8.803088803088803,
      "grad_norm": 0.2825658917427063,
      "learning_rate": 0.00011242424242424243,
      "loss": 7.1064,
      "step": 290
    },
    {
      "epoch": 8.957528957528957,
      "grad_norm": 0.2409611940383911,
      "learning_rate": 0.00011090909090909092,
      "loss": 7.0948,
      "step": 295
    },
    {
      "epoch": 9.092664092664092,
      "grad_norm": 0.26775336265563965,
      "learning_rate": 0.0001093939393939394,
      "loss": 7.1003,
      "step": 300
    },
    {
      "epoch": 9.247104247104247,
      "grad_norm": 0.3340815603733063,
      "learning_rate": 0.00010787878787878789,
      "loss": 7.1015,
      "step": 305
    },
    {
      "epoch": 9.4015444015444,
      "grad_norm": 0.20941568911075592,
      "learning_rate": 0.00010636363636363637,
      "loss": 7.1003,
      "step": 310
    },
    {
      "epoch": 9.555984555984557,
      "grad_norm": 0.27202364802360535,
      "learning_rate": 0.00010484848484848486,
      "loss": 7.0766,
      "step": 315
    },
    {
      "epoch": 9.71042471042471,
      "grad_norm": 0.19668854773044586,
      "learning_rate": 0.00010333333333333334,
      "loss": 7.0724,
      "step": 320
    },
    {
      "epoch": 9.864864864864865,
      "grad_norm": 0.2637697458267212,
      "learning_rate": 0.00010181818181818181,
      "loss": 7.0957,
      "step": 325
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.34781962633132935,
      "learning_rate": 0.0001003030303030303,
      "loss": 7.1005,
      "step": 330
    },
    {
      "epoch": 10.154440154440154,
      "grad_norm": 0.1715756058692932,
      "learning_rate": 9.87878787878788e-05,
      "loss": 7.0693,
      "step": 335
    },
    {
      "epoch": 10.308880308880308,
      "grad_norm": 0.17804552614688873,
      "learning_rate": 9.727272727272728e-05,
      "loss": 7.0806,
      "step": 340
    },
    {
      "epoch": 10.463320463320464,
      "grad_norm": 0.25881731510162354,
      "learning_rate": 9.575757575757576e-05,
      "loss": 7.0831,
      "step": 345
    },
    {
      "epoch": 10.617760617760618,
      "grad_norm": 0.2592390179634094,
      "learning_rate": 9.424242424242424e-05,
      "loss": 7.1028,
      "step": 350
    },
    {
      "epoch": 10.772200772200772,
      "grad_norm": 0.2547719478607178,
      "learning_rate": 9.272727272727273e-05,
      "loss": 7.0835,
      "step": 355
    },
    {
      "epoch": 10.926640926640927,
      "grad_norm": 0.2274358868598938,
      "learning_rate": 9.121212121212121e-05,
      "loss": 7.1051,
      "step": 360
    },
    {
      "epoch": 11.061776061776062,
      "grad_norm": 0.206587553024292,
      "learning_rate": 8.96969696969697e-05,
      "loss": 7.0841,
      "step": 365
    },
    {
      "epoch": 11.216216216216216,
      "grad_norm": 0.27788880467414856,
      "learning_rate": 8.818181818181818e-05,
      "loss": 7.0826,
      "step": 370
    },
    {
      "epoch": 11.37065637065637,
      "grad_norm": 0.2881242036819458,
      "learning_rate": 8.666666666666667e-05,
      "loss": 7.0685,
      "step": 375
    },
    {
      "epoch": 11.525096525096526,
      "grad_norm": 0.2679707407951355,
      "learning_rate": 8.515151515151515e-05,
      "loss": 7.0801,
      "step": 380
    },
    {
      "epoch": 11.67953667953668,
      "grad_norm": 0.26981809735298157,
      "learning_rate": 8.363636363636364e-05,
      "loss": 7.0812,
      "step": 385
    },
    {
      "epoch": 11.833976833976834,
      "grad_norm": 0.25792402029037476,
      "learning_rate": 8.212121212121212e-05,
      "loss": 7.0914,
      "step": 390
    },
    {
      "epoch": 11.988416988416988,
      "grad_norm": 0.31866803765296936,
      "learning_rate": 8.060606060606061e-05,
      "loss": 7.1082,
      "step": 395
    },
    {
      "epoch": 12.123552123552123,
      "grad_norm": 0.24441027641296387,
      "learning_rate": 7.90909090909091e-05,
      "loss": 7.1028,
      "step": 400
    },
    {
      "epoch": 12.277992277992277,
      "grad_norm": 0.23046500980854034,
      "learning_rate": 7.757575757575758e-05,
      "loss": 7.0655,
      "step": 405
    },
    {
      "epoch": 12.432432432432432,
      "grad_norm": 0.21102482080459595,
      "learning_rate": 7.606060606060607e-05,
      "loss": 7.0944,
      "step": 410
    },
    {
      "epoch": 12.586872586872587,
      "grad_norm": 0.26068755984306335,
      "learning_rate": 7.454545454545455e-05,
      "loss": 7.0778,
      "step": 415
    },
    {
      "epoch": 12.741312741312742,
      "grad_norm": 0.20487573742866516,
      "learning_rate": 7.303030303030304e-05,
      "loss": 7.0759,
      "step": 420
    },
    {
      "epoch": 12.895752895752896,
      "grad_norm": 0.20363683998584747,
      "learning_rate": 7.151515151515152e-05,
      "loss": 7.0933,
      "step": 425
    },
    {
      "epoch": 13.03088803088803,
      "grad_norm": 0.2250947207212448,
      "learning_rate": 7e-05,
      "loss": 7.0631,
      "step": 430
    },
    {
      "epoch": 13.185328185328185,
      "grad_norm": 0.2244061976671219,
      "learning_rate": 6.848484848484848e-05,
      "loss": 7.0808,
      "step": 435
    },
    {
      "epoch": 13.339768339768339,
      "grad_norm": 0.2120824009180069,
      "learning_rate": 6.696969696969696e-05,
      "loss": 7.0666,
      "step": 440
    },
    {
      "epoch": 13.494208494208495,
      "grad_norm": 0.2913255989551544,
      "learning_rate": 6.545454545454546e-05,
      "loss": 7.0964,
      "step": 445
    },
    {
      "epoch": 13.64864864864865,
      "grad_norm": 0.25331249833106995,
      "learning_rate": 6.393939393939395e-05,
      "loss": 7.0834,
      "step": 450
    },
    {
      "epoch": 13.803088803088803,
      "grad_norm": 0.2182593047618866,
      "learning_rate": 6.242424242424243e-05,
      "loss": 7.0919,
      "step": 455
    },
    {
      "epoch": 13.957528957528957,
      "grad_norm": 0.20954175293445587,
      "learning_rate": 6.090909090909091e-05,
      "loss": 7.0783,
      "step": 460
    },
    {
      "epoch": 14.092664092664092,
      "grad_norm": 0.18797746300697327,
      "learning_rate": 5.93939393939394e-05,
      "loss": 7.0733,
      "step": 465
    },
    {
      "epoch": 14.247104247104247,
      "grad_norm": 0.17361122369766235,
      "learning_rate": 5.787878787878788e-05,
      "loss": 7.0881,
      "step": 470
    },
    {
      "epoch": 14.4015444015444,
      "grad_norm": 0.20694595575332642,
      "learning_rate": 5.636363636363636e-05,
      "loss": 7.0948,
      "step": 475
    },
    {
      "epoch": 14.555984555984557,
      "grad_norm": 0.15945695340633392,
      "learning_rate": 5.484848484848485e-05,
      "loss": 7.0888,
      "step": 480
    },
    {
      "epoch": 14.71042471042471,
      "grad_norm": 0.20899698138237,
      "learning_rate": 5.333333333333333e-05,
      "loss": 7.0765,
      "step": 485
    },
    {
      "epoch": 14.864864864864865,
      "grad_norm": 0.21584869921207428,
      "learning_rate": 5.181818181818182e-05,
      "loss": 7.072,
      "step": 490
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.3014868199825287,
      "learning_rate": 5.030303030303031e-05,
      "loss": 7.0592,
      "step": 495
    },
    {
      "epoch": 15.154440154440154,
      "grad_norm": 0.3651902377605438,
      "learning_rate": 4.878787878787879e-05,
      "loss": 7.0849,
      "step": 500
    },
    {
      "epoch": 15.308880308880308,
      "grad_norm": 0.20425429940223694,
      "learning_rate": 4.7272727272727275e-05,
      "loss": 7.0654,
      "step": 505
    },
    {
      "epoch": 15.463320463320464,
      "grad_norm": 0.20262733101844788,
      "learning_rate": 4.575757575757576e-05,
      "loss": 7.0619,
      "step": 510
    },
    {
      "epoch": 15.617760617760618,
      "grad_norm": 0.16341835260391235,
      "learning_rate": 4.4242424242424246e-05,
      "loss": 7.0859,
      "step": 515
    },
    {
      "epoch": 15.772200772200772,
      "grad_norm": 0.22196553647518158,
      "learning_rate": 4.2727272727272724e-05,
      "loss": 7.0723,
      "step": 520
    },
    {
      "epoch": 15.926640926640927,
      "grad_norm": 0.18904587626457214,
      "learning_rate": 4.1212121212121216e-05,
      "loss": 7.0949,
      "step": 525
    },
    {
      "epoch": 16.06177606177606,
      "grad_norm": 0.16911384463310242,
      "learning_rate": 3.96969696969697e-05,
      "loss": 7.102,
      "step": 530
    },
    {
      "epoch": 16.216216216216218,
      "grad_norm": 0.16145338118076324,
      "learning_rate": 3.818181818181819e-05,
      "loss": 7.0866,
      "step": 535
    },
    {
      "epoch": 16.37065637065637,
      "grad_norm": 0.21816593408584595,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 7.064,
      "step": 540
    },
    {
      "epoch": 16.525096525096526,
      "grad_norm": 0.14512619376182556,
      "learning_rate": 3.515151515151515e-05,
      "loss": 7.0678,
      "step": 545
    },
    {
      "epoch": 16.679536679536678,
      "grad_norm": 0.21780522167682648,
      "learning_rate": 3.3636363636363636e-05,
      "loss": 7.0786,
      "step": 550
    },
    {
      "epoch": 16.833976833976834,
      "grad_norm": 0.23329080641269684,
      "learning_rate": 3.212121212121212e-05,
      "loss": 7.0923,
      "step": 555
    },
    {
      "epoch": 16.98841698841699,
      "grad_norm": 0.2583337128162384,
      "learning_rate": 3.060606060606061e-05,
      "loss": 7.0656,
      "step": 560
    },
    {
      "epoch": 17.123552123552123,
      "grad_norm": 0.18363161385059357,
      "learning_rate": 2.909090909090909e-05,
      "loss": 7.0592,
      "step": 565
    },
    {
      "epoch": 17.27799227799228,
      "grad_norm": 0.21890127658843994,
      "learning_rate": 2.7575757575757578e-05,
      "loss": 7.0784,
      "step": 570
    },
    {
      "epoch": 17.43243243243243,
      "grad_norm": 0.1302528828382492,
      "learning_rate": 2.6060606060606063e-05,
      "loss": 7.0857,
      "step": 575
    },
    {
      "epoch": 17.586872586872587,
      "grad_norm": 0.1980884075164795,
      "learning_rate": 2.4545454545454545e-05,
      "loss": 7.0909,
      "step": 580
    },
    {
      "epoch": 17.74131274131274,
      "grad_norm": 0.19207750260829926,
      "learning_rate": 2.3030303030303034e-05,
      "loss": 7.0622,
      "step": 585
    },
    {
      "epoch": 17.895752895752896,
      "grad_norm": 0.18000146746635437,
      "learning_rate": 2.1515151515151516e-05,
      "loss": 7.0774,
      "step": 590
    },
    {
      "epoch": 18.030888030888033,
      "grad_norm": 0.1533072292804718,
      "learning_rate": 2e-05,
      "loss": 7.0844,
      "step": 595
    },
    {
      "epoch": 18.185328185328185,
      "grad_norm": 0.13561028242111206,
      "learning_rate": 1.8484848484848487e-05,
      "loss": 7.0922,
      "step": 600
    },
    {
      "epoch": 18.33976833976834,
      "grad_norm": 0.21151481568813324,
      "learning_rate": 1.6969696969696972e-05,
      "loss": 7.081,
      "step": 605
    },
    {
      "epoch": 18.494208494208493,
      "grad_norm": 0.2662647068500519,
      "learning_rate": 1.5454545454545454e-05,
      "loss": 7.0779,
      "step": 610
    },
    {
      "epoch": 18.64864864864865,
      "grad_norm": 0.1993214190006256,
      "learning_rate": 1.3939393939393942e-05,
      "loss": 7.0796,
      "step": 615
    },
    {
      "epoch": 18.8030888030888,
      "grad_norm": 0.20043624937534332,
      "learning_rate": 1.2424242424242424e-05,
      "loss": 7.0639,
      "step": 620
    },
    {
      "epoch": 18.957528957528957,
      "grad_norm": 0.1238618865609169,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 7.0738,
      "step": 625
    },
    {
      "epoch": 19.092664092664094,
      "grad_norm": 0.1861596554517746,
      "learning_rate": 9.393939393939394e-06,
      "loss": 7.0784,
      "step": 630
    },
    {
      "epoch": 19.247104247104247,
      "grad_norm": 0.1764141172170639,
      "learning_rate": 7.878787878787878e-06,
      "loss": 7.0819,
      "step": 635
    },
    {
      "epoch": 19.401544401544403,
      "grad_norm": 0.20354841649532318,
      "learning_rate": 6.363636363636363e-06,
      "loss": 7.0594,
      "step": 640
    },
    {
      "epoch": 19.555984555984555,
      "grad_norm": 0.18009771406650543,
      "learning_rate": 4.848484848484849e-06,
      "loss": 7.0737,
      "step": 645
    },
    {
      "epoch": 19.71042471042471,
      "grad_norm": 0.23348741233348846,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 7.0887,
      "step": 650
    },
    {
      "epoch": 19.864864864864863,
      "grad_norm": 0.16679516434669495,
      "learning_rate": 1.818181818181818e-06,
      "loss": 7.0826,
      "step": 655
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.3409392237663269,
      "learning_rate": 3.0303030303030305e-07,
      "loss": 7.0457,
      "step": 660
    }
  ],
  "logging_steps": 5,
  "max_steps": 660,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.274124900794368e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
